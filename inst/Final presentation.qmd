---
title: "Feature selection in imbalanced data by LASSO"
subtitle: "PHS 7045: Advanced Programming"
author: "Haojia Li, MS"
format:
    revealjs:
      embed-resources: true
      slide-number: true
      scrollable: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
slides_eval <- TRUE
```

## a. Overview of `ImbLassoRcpp`

::: {style="font-size: 20pt"}

The goal of `ImbLassoRcpp` package is to solve feature selection problem by LASSO in data with imbalanced distributed binary outcome by employing stratified cross-validation and/or SMOTE algorithm.

:::

## b. Main Functions

::: {style="font-size: 20pt"}

The `stratified_cv_smote` function prepares the data of cross-validation for parameter tuning of $\lambda$ in LASSO regression. It provides options of whether to use stratified cross-validation on the input data and/or SMOTE algorithm on the training sets.

```{r, eval=FALSE}
stratified_cv_smote(
  X,               # feature matrix
  y,               # binary outcome
  k_cv       = 10, # number of folds for the cross-validation
  stratified = T,  # whether to apply stratified CV
  SMOTE      = T,  # whether to apply SMOTE oversampling
  k_nn       = 5,  # number of nearest neighbors to be considered in SMOTE
  N          = 9,  # number of new synthetic examples to be generated for each observation in SMOTE
  R          = 1   # size ratio of the majority class to be sampled to the SMOTEd minority class
)
```

`stratified` controls the event rate in each fold, while `SMOTE` decides whether to use oversampling approach and synthesis new examples.

There are four combinations of `stratified` and `SMOTE`. When `stratified = F` and `SMOTE = F`, the result is the same as the default settings in a normal CV LASSO.

`par_smote_cv_lasso` will then conduct parallel computing to perform k-fold cross-validation for LASSO regression and complete the parameter tuning of $\lambda$.

`summary.cv.smote.lasso` returns multiple results, including the feature selection result.

:::

## c. Problem to solve (update for package)

::: {style="font-size: 20pt"}

We want to compare the feature selection results across the four different combinations of `stratified` and `SMOTE`.

To do this, we are going to set up a simulation analysis. The job will be submitted by `slurmR`.

The simulation analysis will be added to the package as a vignette.

:::

## d. Solution - 1. Set up

::: {style="font-size: 20pt"}

```{r}
# number of simulations
B <- 50
# number of observations
N <- 50000
# number of features
d <- 50
# proportion of event
prop <- 0.01

# simulate predictive features
X <- MASS::mvrnorm(
  n = N,
  mu = rep(0,5),
  Sigma = matrix(c(
    # Z1,  Z2,  Z3,  Z4,  Z5
    1.0, 0.0, 0.2, 0.0, 0.0, # Z1
    0.0, 1.0, 0.0, 0.2, 0.0, # Z2
    0.2, 0.0, 1.0, 0.0, 0.0, # Z3
    0.0, 0.2, 0.0, 1.0, 0.0, # Z4
    0.0, 0.0, 0.0, 0.0, 1.0  # Z5
  ), nrow = 5, byrow = T)
)

# objective function (loss function)
fun <- function(b, target_prop) {
  Y <- rbinom(100000, 1, prob = plogis(b[1] + X %*% b[2:6] + X[,1]*X[,2]*b[7]))
  (mean(Y) - target_prop)^2
}

# finding the closest value
b <- optim(par = rep(0,7), fn = fun, target_prop = prop)$par

```

:::

## d. Solution - 2. Function

::: {style="font-size: 20pt"}

```{r, eval=FALSE}
library(ImbLassoRcpp)
imb_feature_selection <- function(i, N, d) {
  X <- MASS::mvrnorm(
    n = N,
    mu = rep(0,5),
    Sigma = matrix(c(
      # Z1,  Z2,  Z3,  Z4,  Z5
      1.0, 0.0, 0.2, 0.0, 0.0, # Z1
      0.0, 1.0, 0.0, 0.2, 0.0, # Z2
      0.2, 0.0, 1.0, 0.0, 0.0, # Z3
      0.0, 0.2, 0.0, 1.0, 0.0, # Z4
      0.0, 0.0, 0.0, 0.0, 1.0  # Z5
    ), nrow = 5, byrow = T)
  )
  
  # simulate binary outcome
  Y <- rbinom(N, 1, plogis(b[1] + X %*% b[2:6] + X[,1]*X[,2]*b[7]))
  
  # simulate non-predictive features
  Z <- matrix(rnorm(N*(d-5)), ncol = d-5)
  
  simdata <- data.frame(Y, X, Z)
  colnames(simdata) <- c("Y", paste0("X", 1:5), paste0("Z", 1:(d-5)))
  
  summary.cv.smote.lasso(par_smote_cv_lasso(stratified_cv_smote(
    as.matrix(simdata[,-1]), simdata[,1], stratified = F, SMOTE = F
  )))$`feature selected`
}
```

:::

## d. Solution - 3. Submit job

::: {style="font-size: 20pt"}

```{r, eval=FALSE}
library(slurmR) # This also loads the parallel package

ans <- Slurm_lapply(
  1:B, imb_feature_selection,
  N = N, d = d,
  export = c("b"),
  njobs = 10,
  mc.cores = 10,
  plan  = "collect",
  tmp_path = "/scratch/general/nfs1/u6034070", # This is where all temp files will be exported
  sbatch_opt = list(
    account = "notchpeak-shared-short",
    partition = "notchpeak-shared-short"
  )
)

saveRDS(ans, "Sim50_d50_prop1perc_FF.rds")
```

:::

## e. Result

::: {style="font-size: 20pt"}

Out of 50 simulations, some return the feature selection results while the others return the error.

```{r, eval = F}
[1] "Error in serverSocket(port = port) : \n  creation of server socket failed: port 11722 cannot be opened\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in serverSocket(port = port): creation of server socket failed: port 11722 cannot be opened>
```

The number of simulations successfully returning results and the number and percentage of those select all the five predictive variables are shown below:

+-----------------------+----------------+----------------+----------------+----------------+
|                       | Stratified = F | Stratified = T | Stratified = F | Stratified = T |
|                       |                |                |                |                |
|                       | SMOTE = F      | SMOTE = F      | SMOTE = T      | SMOTE = T      |
+=======================+================+================+================+================+
| N. of success         | 10             | 10             | 15             | 16             |
+-----------------------+----------------+----------------+----------------+----------------+
| N. of all 5 selection | 10 (100%)      | 4 (40%)        | 13 (87%)       | 16 (100%)      |
+-----------------------+----------------+----------------+----------------+----------------+

:::

# Thanks!
