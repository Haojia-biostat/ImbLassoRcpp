#' Parameter tuning for LASSO using parallel computing
#'
#' This function employs parallel computing to perform k-fold cross-validation for LASSO regression \
#' on datasets with an imbalanced binary outcome, enabling the tuning of the regularization parameter,\
#'  \eqn{\lambda}.
#'
#' @param X feature matrix or \code{cv_smote_data} generated by function \code{stratified_cv_smote}
#' @param y binary outcome, where 1 = positive event & minority class and 0 = negative event & majority class;\
#'  ignored if \code{x} is \code{cv_smote_data}
#' @param nlambda number of \code{lambda} values, default value is 100; ignored if \code{lambda} is supplied by user
#' @param lambda \code{lambda} sequence, typically computed by program based on \code{nlambda} and input data
#' @param \dots other arguments that can be passed to \code{stratified_cv_smote} or \code{glmnet}
#'
#' @return An object with class \code{"cv_smote_lasso"} including:
#' \item{X}{input - feature matrix}
#' \item{y}{input - binary outcome}
#' \item{lambda}{input or generated by program - lambda sequence}
#' \item{deviance}{matrix with \code{k} columns and \code{nlambda} rows, where each column \
#' contains the deviance, calculated as \eqn{-2\times \sum(ylog(p) + (1-y)log(1-p))}, for each fold}
#'
#' @examples
#' set.seed(1010)
#' n = 1000
#' p = 100
#' nzc = trunc(p/10)
#' x = matrix(rnorm(n * p), n, p)
#' beta = rnorm(nzc)
#' fx = x[, seq(nzc)] %*% beta
#' px = exp(fx)
#' px = px/(1 + px)
#' y = rbinom(n = length(px), prob = px, size = 1)
#' fit <- par_smote_cv_lasso(x, y, stratified = FALSE, SMOTE = FALSE)
#'
#' @export

par_smote_cv_lasso <- function(
    X,
    y = NULL,
    nlambda = 100,
    lambda = NULL,
    ...
) {
  # check whether input X is a featured list or a matrix
  if(!(methods::is(X, "cv_smote_data"))) {
    if(!is.matrix(X)) {
      if(is.data.frame(X))
        X <- as.matrix(X)
      else
        stop("X should be either a cv_smote_data or a matrix.")
    } else {
      if(is.null(y))
        stop("Input y is required given input X is a matrix.")
      else
        X <- stratified_cv_smote(X, y, ...)
    }
  }

  X1 <- do.call(rbind, X$test) |> as.matrix()
  if(is.null(lambda)) lambda <- lambda_gen(X1[, -ncol(X1)], X1[, ncol(X1)], K = nlambda)$Lambdas

  # define function for the calculation of deviance
  binom_dev <- function(i) {
    testdata <- X$test[[i]]
    traindata <- X$train[[i]]
    nX <- ncol(testdata) - 1
    fit <- glmnet::glmnet(traindata[, 1:nX], traindata[, nX+1], family = "binomial", lambda = lambda, ...)
    pred <- stats::predict(fit, newx = as.matrix(testdata[, 1:nX]), s = lambda, type = "response")
    y <- testdata[, nX+1]
    apply(pred, 2, \(x) -2*(sum(y*log(x) + (1-y)*(log(1-x)))))
  }

  # do parallel computing
  cl <- parallel::makeCluster(2)
  on.exit(parallel::stopCluster(cl))
  parallel::clusterExport(cl, c("X", "binom_dev"), envir = environment())
  binom_dev_mat <- parallel::parSapply(cl, seq(length(X$test)), binom_dev)

  # generate output list
  res <- vector(mode = "list", length = 4)
  names(res) <- c("X", "y", "lambda", "deviance")

  res$X <- X1[, -ncol(X1)]
  res$y <- X1[, ncol(X1)]
  res$lambda <- lambda
  res$deviance <- as.data.frame(binom_dev_mat) |> stats::setNames(paste0("k", seq(length(X$test))))

  # define output class
  class(res) <- c("cv_smote_lasso", "list")
  return(res)
}

